{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51cb4063",
   "metadata": {},
   "source": [
    "# Aprendizaje profundo para detección de sexismo\n",
    "- Óscar Alvarado\n",
    "- Dante Bermúdez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "842ed64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from itertools import islice as take\n",
    "import torch\n",
    "from torch import nn\n",
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# barras de progreso\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3a03b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add98357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colapsar_repeticion(match):\n",
    "    elemento = match.groups()\n",
    "    assert len(match.groups()) == 1\n",
    "    return elemento[0]\n",
    "\n",
    "def procesar_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    regex_usuario = re.compile(r\"@[\\w\\d]+\")\n",
    "    tweet = regex_usuario.sub(\"@usuario\", tweet)\n",
    "    \n",
    "    regex_link = re.compile(r\"\\b(?:https?://|www\\.)\\S+\\b\")\n",
    "    tweet = regex_link.sub(\"<link>\", tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    regex_collapse = re.compile(r\"(\\w)\\1{2}\")\n",
    "    \n",
    "    tokens = [regex_collapse.sub(colapsar_repeticion, token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf553a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be04ed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>She calls herself \"anti-feminazi\" how about sh...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>2</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>Now, back to these women, the brave and the be...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@CurvyBandida @Xalynne_B Wow, your skirt is ve...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>4</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@AurelieGuiboud Incredible!  Beautiful!But I l...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>5</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>i find it extremely hard to believe that kelly...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_case  id   source language  \\\n",
       "0  EXIST2021   1  twitter       en   \n",
       "1  EXIST2021   2  twitter       en   \n",
       "2  EXIST2021   3  twitter       en   \n",
       "3  EXIST2021   4  twitter       en   \n",
       "4  EXIST2021   5  twitter       en   \n",
       "\n",
       "                                                text       task1  \\\n",
       "0  She calls herself \"anti-feminazi\" how about sh...      sexist   \n",
       "1  Now, back to these women, the brave and the be...  non-sexist   \n",
       "2  @CurvyBandida @Xalynne_B Wow, your skirt is ve...      sexist   \n",
       "3  @AurelieGuiboud Incredible!  Beautiful!But I l...  non-sexist   \n",
       "4  i find it extremely hard to believe that kelly...  non-sexist   \n",
       "\n",
       "                    task2  \n",
       "0  ideological-inequality  \n",
       "1              non-sexist  \n",
       "2         objectification  \n",
       "3              non-sexist  \n",
       "4              non-sexist  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../datos/training/EXIST2021_training.tsv\", sep=\"\\t\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718d54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_train[\"text\"].apply(procesar_tweet)\n",
    "labels1 = df_train[\"task1\"].map({\"sexist\":1, \"non-sexist\":0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308f5b1",
   "metadata": {},
   "source": [
    "## Train - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b015701",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets, labels1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040f559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5581x8424 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 135091 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=2, tokenizer=lambda x:x, preprocessor=lambda x:x)\n",
    "vect.fit_transform(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20221490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '#16días',\n",
       " '#16díasdeactivismo',\n",
       " '#4dic',\n",
       " '#abortolegal',\n",
       " '#abortolegal2020',\n",
       " '#abortolegalesvida',\n",
       " '#absabhogefit',\n",
       " '#accesibilidad',\n",
       " '#amateurporn',\n",
       " '#andrósfera',\n",
       " '#antifeminazi',\n",
       " '#antifeminism',\n",
       " '#antifeminist',\n",
       " '#argentina',\n",
       " '#artwork',\n",
       " '#ass',\n",
       " '#aztecanoticas',\n",
       " '#babygirl',\n",
       " '#bbc',\n",
       " '#bbw',\n",
       " '#bi',\n",
       " '#bigcockslover',\n",
       " '#bigcocksonly',\n",
       " '#bigtits',\n",
       " '#blacklistedhttps',\n",
       " '#blm',\n",
       " '#blowjob',\n",
       " '#bollywood',\n",
       " '#boobs',\n",
       " '#boys',\n",
       " '#bridgerton',\n",
       " '#bullied',\n",
       " '#buyfineart',\n",
       " '#bájalasdelpedestal',\n",
       " '#cdmx',\n",
       " '#chat',\n",
       " '#christmas',\n",
       " '#christmas2020',\n",
       " '#chudai',\n",
       " '#cidh',\n",
       " '#clay',\n",
       " '#cndh',\n",
       " '#cock',\n",
       " '#cocktribute',\n",
       " '#coronavirus',\n",
       " '#covid19',\n",
       " '#cum',\n",
       " '#cumonmyface',\n",
       " '#cumslut',\n",
       " '#cumswallower',\n",
       " '#cumtribute',\n",
       " '#cumtributeko',\n",
       " '#deepthroat',\n",
       " '#derechoshumanos',\n",
       " '#desi',\n",
       " '#desichudai',\n",
       " '#desifuck',\n",
       " '#desigirl',\n",
       " '#desipussy',\n",
       " '#desisex',\n",
       " '#desislut',\n",
       " '#desiwife',\n",
       " '#desiwifeshare',\n",
       " '#dick',\n",
       " '#dickpic',\n",
       " '#dif',\n",
       " '#dm',\n",
       " '#dmme',\n",
       " '#doublepénétration',\n",
       " '#dpme',\n",
       " '#empoderamiento',\n",
       " '#entrepreneur',\n",
       " '#equality',\n",
       " '#erotic',\n",
       " '#esahorasenado',\n",
       " '#esley',\n",
       " '#espectáculos',\n",
       " '#ethereum',\n",
       " '#etsy',\n",
       " '#everydaysexism',\n",
       " '#fashion',\n",
       " '#feminazi',\n",
       " '#feminism',\n",
       " '#feminismisawful',\n",
       " '#feminismiscancer',\n",
       " '#feminismismisandry',\n",
       " '#feminismo',\n",
       " '#feminist',\n",
       " '#feministshatemen',\n",
       " '#fetish',\n",
       " '#fillmewithcum',\n",
       " '#fitness',\n",
       " '#follow',\n",
       " '#free',\n",
       " '#friends',\n",
       " '#fuck',\n",
       " '#fuckme',\n",
       " '#funny',\n",
       " '#gamergate',\n",
       " '#gangbang',\n",
       " '#gangbanged',\n",
       " '#garvm_pharma',\n",
       " '#gay',\n",
       " '#gender',\n",
       " '#genderequality',\n",
       " '#giveaway',\n",
       " '#gobiernodelestado',\n",
       " '#guardianacional',\n",
       " '#guys',\n",
       " '#happyeaster',\n",
       " '#harassed',\n",
       " '#hardcock',\n",
       " '#hentai',\n",
       " '#history',\n",
       " '#hmu',\n",
       " '#homedecor',\n",
       " '#horny',\n",
       " '#hornyaf',\n",
       " '#hornydm',\n",
       " '#hornymilf',\n",
       " '#hornyteen',\n",
       " '#hotwife',\n",
       " '#humanrights',\n",
       " '#ico',\n",
       " '#indian',\n",
       " '#indianhotwife',\n",
       " '#indianpussy',\n",
       " '#indianslut',\n",
       " '#indianwife',\n",
       " '#intimidated',\n",
       " '#jewellery',\n",
       " '#joebiden',\n",
       " '#justicia',\n",
       " '#kamalaharris',\n",
       " '#kincora',\n",
       " '#kindleunlimitedus',\n",
       " '#lacasafuerte20',\n",
       " '#ladies',\n",
       " '#lamanada',\n",
       " '#lgbtq',\n",
       " '#love',\n",
       " '#machismo',\n",
       " \"#man's\",\n",
       " '#manspreading',\n",
       " '#masculinidad',\n",
       " '#masculinity',\n",
       " '#men',\n",
       " '#mensrights',\n",
       " '#mensrightsactivist',\n",
       " '#mensrightsmatter',\n",
       " '#mensrightsmovement',\n",
       " '#mentoo',\n",
       " '#metoo',\n",
       " '#mgtow',\n",
       " '#mgtowhttps',\n",
       " '#michigan',\n",
       " '#milf',\n",
       " '#misogyny',\n",
       " '#monument',\n",
       " '#mujeres',\n",
       " '#nacional',\n",
       " '#naked',\n",
       " '#newmusicfriday',\n",
       " '#niunamenos',\n",
       " '#niunamás',\n",
       " '#noesno',\n",
       " '#notallmen',\n",
       " '#notallwhitepeople',\n",
       " '#noticierostelevisa',\n",
       " '#nsfw',\n",
       " '#nude',\n",
       " '#nudes',\n",
       " '#oneonone',\n",
       " '#onlyfans',\n",
       " '#pathetic',\n",
       " '#patriarchy',\n",
       " '#penis',\n",
       " '#pfp',\n",
       " '#pfpapoya',\n",
       " '#pgj',\n",
       " '#pgr',\n",
       " '#photography',\n",
       " '#pildoraroja',\n",
       " '#police',\n",
       " '#porn',\n",
       " '#portípormíportodas',\n",
       " '#pregnant',\n",
       " '#pregnantandhorny',\n",
       " '#pressforprogress',\n",
       " '#privatetraining',\n",
       " '#prochoice',\n",
       " '#puebla',\n",
       " '#pussy',\n",
       " '#qatarairways',\n",
       " '#quesealey',\n",
       " '#races',\n",
       " '#rainn',\n",
       " '#rationalmale',\n",
       " '#redpill',\n",
       " '#republican',\n",
       " '#respeto',\n",
       " '#rhop',\n",
       " '#rosmello',\n",
       " '#salariorosa',\n",
       " '#sales',\n",
       " '#sculpture',\n",
       " '#sculptures',\n",
       " '#sedena',\n",
       " '#seraley',\n",
       " '#sex',\n",
       " '#sexism',\n",
       " '#sexomentirasycynthiasdevideo',\n",
       " '#sext',\n",
       " '#sexually',\n",
       " '#sexy',\n",
       " '#slut',\n",
       " '#snapme',\n",
       " '#sodomy',\n",
       " '#stalked',\n",
       " '#straight',\n",
       " '#suck',\n",
       " '#sugarbaby',\n",
       " '#sugardaddylegit',\n",
       " '#tbt',\n",
       " '#tease',\n",
       " '#teatro',\n",
       " '#telediario',\n",
       " '#telemundo',\n",
       " '#timesup',\n",
       " '#tits',\n",
       " '#trade',\n",
       " '#tribute',\n",
       " '#ttrpg',\n",
       " '#twitch',\n",
       " '#twittersupport',\n",
       " '#vaw',\n",
       " '#violenciamachista',\n",
       " '#wcw',\n",
       " '#wet',\n",
       " '#wetaf',\n",
       " '#whore',\n",
       " \"#woman's\",\n",
       " '#women',\n",
       " '#womenempowerment',\n",
       " '#womenfashion',\n",
       " '#womens',\n",
       " '#womensclothing',\n",
       " '#womensday',\n",
       " '#womensequalityday',\n",
       " '#womensfashion',\n",
       " '#womenshealth',\n",
       " '#womensrights',\n",
       " '#womensstyle',\n",
       " '#womenstyle',\n",
       " '#womensupportingwomen',\n",
       " '#womenswear',\n",
       " '#yositecreo',\n",
       " '#yosítecreo',\n",
       " '#youtube',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '):',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--->',\n",
       " '.',\n",
       " '. .',\n",
       " '. . .',\n",
       " '. ...',\n",
       " '..',\n",
       " '...',\n",
       " '/',\n",
       " '0',\n",
       " '00',\n",
       " '016',\n",
       " '01819357',\n",
       " '062',\n",
       " '091',\n",
       " '1',\n",
       " '1/2',\n",
       " '10',\n",
       " '100',\n",
       " '10k',\n",
       " '10pm',\n",
       " '10th',\n",
       " '11',\n",
       " '112',\n",
       " '112accesible',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '1950s',\n",
       " '1984',\n",
       " '1987',\n",
       " '1era',\n",
       " '1st',\n",
       " '2',\n",
       " '2/2',\n",
       " '20',\n",
       " '200',\n",
       " '2003',\n",
       " '2006',\n",
       " '2015',\n",
       " '2016',\n",
       " '2018',\n",
       " '2019',\n",
       " '2020',\n",
       " '2020.there',\n",
       " '2021',\n",
       " '2024',\n",
       " '2030',\n",
       " '2077',\n",
       " '20s',\n",
       " '20wks',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '3',\n",
       " '3/3',\n",
       " '30',\n",
       " '300',\n",
       " '31',\n",
       " '35',\n",
       " '3rd',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '44',\n",
       " '45',\n",
       " '48',\n",
       " '4t',\n",
       " '4th',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '52',\n",
       " '6',\n",
       " '60',\n",
       " '600',\n",
       " '63',\n",
       " '65',\n",
       " '7',\n",
       " '70',\n",
       " '72',\n",
       " '75',\n",
       " '7mnths',\n",
       " '8',\n",
       " '80',\n",
       " '80s',\n",
       " '84',\n",
       " '85',\n",
       " '8:',\n",
       " '8m',\n",
       " '9',\n",
       " '90',\n",
       " '900.116',\n",
       " '90s',\n",
       " '911',\n",
       " '92',\n",
       " '99',\n",
       " ':',\n",
       " ':(',\n",
       " ':)',\n",
       " ':-)',\n",
       " ':/',\n",
       " ':d',\n",
       " ':p',\n",
       " ';',\n",
       " ';)',\n",
       " '<',\n",
       " '<3',\n",
       " '<link>',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '@usuario',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " 'a',\n",
       " 'abajo',\n",
       " 'abandono',\n",
       " 'abierta',\n",
       " 'abierto',\n",
       " 'abiertos',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'ableist',\n",
       " 'abnegada',\n",
       " 'abogada',\n",
       " 'abogado',\n",
       " 'aborta',\n",
       " 'abortado',\n",
       " 'abortan',\n",
       " 'abortar',\n",
       " 'aborted',\n",
       " 'aborten',\n",
       " 'abortera',\n",
       " 'aborteras',\n",
       " 'abortion',\n",
       " 'aborto',\n",
       " 'abortos',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abrazo',\n",
       " 'abre',\n",
       " 'abrir',\n",
       " 'abrirme',\n",
       " 'abro',\n",
       " 'absolución',\n",
       " 'absolutamente',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absoluto',\n",
       " 'absuelve',\n",
       " 'absurdo',\n",
       " 'abt',\n",
       " 'abuela',\n",
       " 'abuelo',\n",
       " 'abusadas',\n",
       " 'abusan',\n",
       " 'abusar',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abusers',\n",
       " 'abusing',\n",
       " 'abusiva',\n",
       " 'abusive',\n",
       " 'abuso',\n",
       " 'abusos',\n",
       " 'abusó',\n",
       " 'aca',\n",
       " 'acaba',\n",
       " 'acaban',\n",
       " 'acabar',\n",
       " 'acabas',\n",
       " 'acabe',\n",
       " 'acabo',\n",
       " 'acabó',\n",
       " 'academia',\n",
       " 'acaso',\n",
       " 'acceder',\n",
       " 'accept',\n",
       " 'acceso',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidente',\n",
       " 'acciones',\n",
       " 'acción',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accounts',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'acepta',\n",
       " 'aceptable',\n",
       " 'aceptar',\n",
       " 'acerca',\n",
       " 'acercó',\n",
       " 'achieve',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'acknowledge',\n",
       " 'aclara',\n",
       " 'acompañante',\n",
       " 'acordado',\n",
       " 'acordar',\n",
       " 'acosar',\n",
       " 'acoso',\n",
       " 'acostumbrados',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'actitud',\n",
       " 'actitudes',\n",
       " 'actively',\n",
       " 'actividad',\n",
       " 'actividades',\n",
       " 'activismo',\n",
       " 'activist',\n",
       " 'activistas',\n",
       " 'activists',\n",
       " 'acto',\n",
       " 'actor',\n",
       " 'actos',\n",
       " 'actrices',\n",
       " 'actriz',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actualidad',\n",
       " 'actually',\n",
       " 'actualmente',\n",
       " 'actuar',\n",
       " 'actúa',\n",
       " 'acuario',\n",
       " 'acuerda',\n",
       " 'acuerdo',\n",
       " 'acusa',\n",
       " 'acusación',\n",
       " 'acusado',\n",
       " 'acusados',\n",
       " 'acuña',\n",
       " 'acá',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'adecuadas',\n",
       " 'adelante',\n",
       " 'ademas',\n",
       " 'además',\n",
       " 'administración',\n",
       " 'admire',\n",
       " 'admiro',\n",
       " 'admit',\n",
       " 'admitir',\n",
       " 'admits',\n",
       " 'adn',\n",
       " 'adoctrinamiento',\n",
       " 'adolescente',\n",
       " 'adolescentes',\n",
       " 'adorable',\n",
       " 'adult',\n",
       " 'adultas',\n",
       " 'adults',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advantage',\n",
       " 'adventures',\n",
       " 'advertencia',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'advocating',\n",
       " 'aesthetics',\n",
       " 'af',\n",
       " 'afecta',\n",
       " 'afectadas',\n",
       " 'afectados',\n",
       " 'affecting',\n",
       " 'affirms',\n",
       " 'afford',\n",
       " 'afición',\n",
       " 'afirmación',\n",
       " 'afirmar',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agarrar',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agenda',\n",
       " 'ages',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'ago',\n",
       " 'agradezco',\n",
       " 'agredan',\n",
       " 'agreden',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agresiones',\n",
       " 'agresivos',\n",
       " 'agresión',\n",
       " 'agresor',\n",
       " 'agresores',\n",
       " 'agricultura',\n",
       " 'agua',\n",
       " 'aguanta',\n",
       " 'aguantar',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahi',\n",
       " 'ahora',\n",
       " 'ahre',\n",
       " 'ahí',\n",
       " 'aide',\n",
       " 'aim',\n",
       " 'ain',\n",
       " \"ain't\",\n",
       " 'aint',\n",
       " 'air',\n",
       " 'aire',\n",
       " 'aisa',\n",
       " 'aislamiento',\n",
       " 'ajena',\n",
       " 'ajeno',\n",
       " 'ajá',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'alarma',\n",
       " 'alas',\n",
       " 'alba',\n",
       " 'alberto',\n",
       " 'album',\n",
       " 'alcalde',\n",
       " 'alcaldesa',\n",
       " 'alcance',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alegre',\n",
       " 'alegría',\n",
       " 'alejado',\n",
       " 'alejandro',\n",
       " 'alejate',\n",
       " 'alejense',\n",
       " 'alert',\n",
       " 'alerta',\n",
       " 'alertcops',\n",
       " 'alexa',\n",
       " 'alfa',\n",
       " 'algo',\n",
       " 'alguien',\n",
       " 'alguna',\n",
       " 'algunas',\n",
       " 'alguno',\n",
       " 'algunos',\n",
       " 'algún',\n",
       " 'aliados',\n",
       " 'alias',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'alimentos',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allah',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allies',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'ally',\n",
       " 'allá',\n",
       " 'allí',\n",
       " 'alma',\n",
       " 'almirante',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'already',\n",
       " 'alrededor',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alta',\n",
       " 'altas',\n",
       " 'alter',\n",
       " 'alternativa',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'altura',\n",
       " 'alv',\n",
       " 'always',\n",
       " 'am',\n",
       " 'ama',\n",
       " 'amable',\n",
       " 'amado',\n",
       " 'amamos',\n",
       " 'aman',\n",
       " 'amante',\n",
       " 'amantes',\n",
       " 'amar',\n",
       " 'amarillo',\n",
       " 'amateur',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'ambientalismo',\n",
       " 'ambiente',\n",
       " 'ambos',\n",
       " 'amen',\n",
       " 'amenazas',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'ameritan',\n",
       " 'amiga',\n",
       " 'amigas',\n",
       " 'amigo',\n",
       " 'amigos',\n",
       " 'amistad',\n",
       " 'amistades',\n",
       " 'amo',\n",
       " 'among',\n",
       " 'amor',\n",
       " 'amount',\n",
       " 'amparo',\n",
       " 'ampliación',\n",
       " 'amplio',\n",
       " 'amsterdam',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'analfabeto',\n",
       " 'analysis',\n",
       " 'anand',\n",
       " 'ancianos',\n",
       " 'ancient',\n",
       " 'and',\n",
       " 'anda',\n",
       " 'andan',\n",
       " 'andar',\n",
       " 'andas',\n",
       " 'ando',\n",
       " 'androcentrismo',\n",
       " 'angela',\n",
       " 'angelito',\n",
       " 'anger',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animales',\n",
       " 'animalitos',\n",
       " 'animals',\n",
       " 'anime',\n",
       " 'anita',\n",
       " 'annoy',\n",
       " 'annoying',\n",
       " 'anoche',\n",
       " 'anonymity',\n",
       " 'anormal',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'ante',\n",
       " 'anterior',\n",
       " 'antes',\n",
       " 'anti',\n",
       " 'anti-trans',\n",
       " 'anticonceptivos',\n",
       " 'antifa',\n",
       " 'antigua',\n",
       " 'antigüedad',\n",
       " 'antonio',\n",
       " 'anunció',\n",
       " 'any',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'análisis',\n",
       " 'aoc',\n",
       " 'aparece',\n",
       " 'aparecen',\n",
       " 'aparecer',\n",
       " 'apart',\n",
       " 'aparte',\n",
       " 'apellido',\n",
       " 'apenas',\n",
       " 'aplaude',\n",
       " 'aplaudiendo',\n",
       " 'aplicación',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apostado',\n",
       " 'apoya',\n",
       " 'apoyan',\n",
       " 'apoyar',\n",
       " 'apoyo',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appearance',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appointments',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'appropriate',\n",
       " 'aprendan',\n",
       " 'aprender',\n",
       " 'aprendido',\n",
       " 'aprendimos',\n",
       " 'aprieta',\n",
       " 'aprobación',\n",
       " 'aprobar',\n",
       " 'aprovecha',\n",
       " 'aprovecharse',\n",
       " 'aprox',\n",
       " 'apuesta',\n",
       " 'apuesto',\n",
       " 'aquel',\n",
       " 'aquella',\n",
       " 'aquellas',\n",
       " 'aquello',\n",
       " 'aquellos',\n",
       " 'aqui',\n",
       " 'aquí',\n",
       " 'arabia',\n",
       " 'arbitros',\n",
       " 'archeological',\n",
       " 'arda',\n",
       " 'ardent',\n",
       " 'arder',\n",
       " 'are',\n",
       " 'area',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arg',\n",
       " 'argentina',\n",
       " 'argentinas',\n",
       " 'argentinos',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'argumento',\n",
       " 'argumentos',\n",
       " 'arguments',\n",
       " 'ariana',\n",
       " 'arma',\n",
       " 'armadas',\n",
       " 'armchairs',\n",
       " 'arms',\n",
       " 'around',\n",
       " 'arrancar',\n",
       " 'arrastrada',\n",
       " 'arrested',\n",
       " 'arriaza',\n",
       " 'arriba',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrogant',\n",
       " 'arse',\n",
       " 'art',\n",
       " 'arte',\n",
       " 'artes',\n",
       " 'arteta',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artist',\n",
       " 'artista',\n",
       " 'artistas',\n",
       " 'artists',\n",
       " 'artículo',\n",
       " 'as',\n",
       " 'asamblea',\n",
       " 'asco',\n",
       " 'asegura',\n",
       " 'asegurar',\n",
       " 'aseguro',\n",
       " 'asesinar',\n",
       " 'asesinas',\n",
       " 'asesinato',\n",
       " 'asesinatos',\n",
       " 'asesino',\n",
       " 'asexual',\n",
       " 'asf',\n",
       " 'ashamed',\n",
       " 'asi',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'asiento',\n",
       " 'asimilar',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'aspecto',\n",
       " 'aspectos',\n",
       " 'asquerosa',\n",
       " 'asqueroso',\n",
       " 'ass',\n",
       " 'assault',\n",
       " 'assaulted',\n",
       " 'asses',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'assistance',\n",
       " 'associate',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assumptions',\n",
       " 'asu',\n",
       " 'asumir',\n",
       " 'asuntos',\n",
       " 'asustense',\n",
       " 'así',\n",
       " 'at',\n",
       " 'ataca',\n",
       " 'atacan',\n",
       " 'atacar',\n",
       " 'ataque',\n",
       " 'ate',\n",
       " 'atencion',\n",
       " 'atención',\n",
       " 'atender',\n",
       " 'atenpro',\n",
       " 'atheist',\n",
       " 'atiende',\n",
       " 'atm',\n",
       " 'atractiva',\n",
       " 'atras',\n",
       " 'atreves',\n",
       " 'atrás',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacking',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'audacity',\n",
       " 'audición',\n",
       " 'audience',\n",
       " 'auge',\n",
       " 'aumentar',\n",
       " 'aun',\n",
       " 'aunque',\n",
       " 'aura',\n",
       " 'austen',\n",
       " 'australia',\n",
       " 'authorities',\n",
       " 'autistic',\n",
       " 'automatically',\n",
       " 'autonomy',\n",
       " 'autores',\n",
       " 'auténtica',\n",
       " 'available',\n",
       " 'avance',\n",
       " 'avanzado',\n",
       " 'avanzar',\n",
       " 'average',\n",
       " 'avisar',\n",
       " 'avión',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'aww',\n",
       " 'ay',\n",
       " 'aye',\n",
       " 'ayer',\n",
       " 'ayuda',\n",
       " 'ayudan',\n",
       " 'ayudar',\n",
       " 'ayudarte',\n",
       " 'ayuso',\n",
       " 'azotar',\n",
       " 'azul',\n",
       " 'azules',\n",
       " 'año',\n",
       " 'años',\n",
       " 'aún',\n",
       " 'b',\n",
       " 'babe',\n",
       " 'babes',\n",
       " 'babies',\n",
       " 'baboso',\n",
       " ...]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ad49f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5714, 8240, 1645, 4982, 4189, 6389, 5405, 1, 7529, 8234, 4183, 7533, 5429, 5425, 6243, 5462, 3990, 2155, 327]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vect.get_feature_names(), 2)}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<START>\"] = 1\n",
    "X_train = [[word2idx.get(word, 1) for word in tweet] for tweet in train_tweets]\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d9bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6794, 397, 7632, 4395, 5108, 7585, 6261, 7469, 4615, 4989, 2365, 2213, 5588, 2313, 2927, 6957, 6308, 418, 276, 770, 5261, 276, 8314, 3594, 2213, 2365, 1, 2618, 8312, 7638, 4615, 6261, 6748, 6308, 1, 2227, 7469, 4615, 4989, 2365, 276, 7011, 657, 6748, 5099, 5946, 7648, 2084, 1985]\n"
     ]
    }
   ],
   "source": [
    "X_test = [[word2idx.get(word, 1) for word in tweet] for tweet in test_tweets]\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da001d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(X) for X in X_train])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2f9b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8426"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras en el vocabulario\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc39f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "for idx, X in enumerate(X_train):\n",
    "    zeros = [0 for _ in range(max_len)]\n",
    "    len_x = len(X)\n",
    "    zeros[-len_x:] = X\n",
    "    X_train[idx] = zeros\n",
    "    \n",
    "for idx, X in enumerate(X_test):\n",
    "    zeros = [0 for _ in range(max_len)]\n",
    "    len_x = len(X)\n",
    "    zeros[-len_x:] = X\n",
    "    X_test[idx] = zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e2638",
   "metadata": {},
   "source": [
    "## Arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ee5a6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de arquitectura\n",
    "class CNN(nn.Module):    \n",
    "    def __init__(self, num_labels=2):\n",
    "        super(CNN, self).__init__()\n",
    "        # Valores iniciales\n",
    "        num_embeddings = 8438\n",
    "        embedding_dim = 100\n",
    "        kernels = 300\n",
    "        k_cnn = 9\n",
    "        pad_cnn = 0\n",
    "        dilation_cnn = 1\n",
    "        step_cnn = 1\n",
    "        k_pool = 1\n",
    "        pad_pool = 0\n",
    "        dilation_pool = 1\n",
    "        step_pool = 1\n",
    "        \n",
    "        # Capa para Embeddings\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim) # 8438 palabras en el vocabulario, embedding 50-dimensional\n",
    "        \n",
    "        # Capa convolucional\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = embedding_dim, out_channels = kernels, kernel_size = k_cnn, padding = pad_cnn,\n",
    "                     dilation = dilation_cnn, stride = step_cnn),\n",
    "            # Función de activación\n",
    "            nn.ReLU(),\n",
    "            # Pooling\n",
    "            nn.MaxPool1d(kernel_size = k_pool, padding = pad_pool,\n",
    "                     dilation = dilation_pool, stride = step_pool))\n",
    "        \n",
    "        # Aplanado\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculando el número de características\n",
    "        out_cnn = int((110 + 2*pad_cnn - dilation_cnn*(k_cnn - 1) - 1)/step_cnn) + 1\n",
    "        out_pool = int((out_cnn + 2*pad_pool - dilation_pool*(k_pool - 1) - 1)/step_pool) + 1\n",
    "        \n",
    "        self.num_features = kernels*out_pool\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #Clasificación\n",
    "        self.cls = nn.Linear(self.num_features, num_labels)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # metodo para inferencia\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.cls(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4d0d7c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (emb): Embedding(8438, 100)\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(100, 300, kernel_size=(9,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (cls): Linear(in_features=30600, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d23c47b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 110]) => torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# inferencia con datos sintéticos\n",
    "x = torch.tensor([X_train[0]])\n",
    "y = model(x)\n",
    "print(f'{x.shape} => {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "740bb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train).to(torch.int64) # transform to torch tensor\n",
    "y_train_t = torch.Tensor(train_labels).to(torch.int64)\n",
    "\n",
    "trn_dataset = TensorDataset(X_train_t, y_train_t) # create your datset\n",
    "trn_dl = DataLoader(trn_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8db1772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = torch.Tensor(X_test).to(torch.int64) # transform to torch tensor\n",
    "y_test_t = torch.Tensor(test_labels).to(torch.int64)\n",
    "\n",
    "tst_dataset = TensorDataset(X_test_t, y_test_t) # create your datset\n",
    "tst_dl = DataLoader(tst_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1f69d",
   "metadata": {},
   "source": [
    "## Modelo paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50d328be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_GPU(dl, model, opt):\n",
    "\n",
    "    # por cada lote\n",
    "    for x, y_true in dl:\n",
    "        \n",
    "        # computamos logits\n",
    "        y_lgts = model(x.to(torch.int64))\n",
    "        \n",
    "        # computamos la pérdida\n",
    "        loss = F.cross_entropy(y_lgts, y_true)\n",
    "        \n",
    "        # vaciamos los gradientes\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # retropropagamos\n",
    "        loss.backward()\n",
    "        \n",
    "        # actualizamos parámetros\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "def eval_epoch_GPU(dl, model, num_batches=None):\n",
    "\n",
    "    # evitamos que se registren las operaciones \n",
    "    # en la gráfica de cómputo\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # historiales\n",
    "        losses, accs = [], []\n",
    "\n",
    "        # validación de la época con num_batches\n",
    "        # si num_batches==None, se usan todos los lotes\n",
    "        for x, y_true in take(dl, num_batches):\n",
    "\n",
    "            # computamos los logits\n",
    "            y_lgts = model(x)\n",
    "\n",
    "            # computamos los puntajes\n",
    "            y_prob = F.softmax(y_lgts, 1)\n",
    "\n",
    "            # computamos la clases\n",
    "            y_pred = torch.argmax(y_prob, 1)\n",
    "\n",
    "            # computamos la pérdida\n",
    "            loss = F.cross_entropy(y_lgts, y_true)\n",
    "\n",
    "            # computamos la exactitud\n",
    "            acc = (y_true == y_pred).type(torch.float32).mean()\n",
    "\n",
    "            # guardamos históricos\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc.item())\n",
    "\n",
    "        # promediamos\n",
    "        loss = np.mean(losses) * 100\n",
    "        acc = np.mean(accs) * 100\n",
    "\n",
    "        return loss, acc\n",
    "        \n",
    "        \n",
    "def train_GPU(model, trn_dl, tst_dl, lr=1e-3, epochs=20,\n",
    "          trn_batches=None, tst_batches=None):\n",
    "\n",
    "    # historiales\n",
    "    loss_hist, acc_hist = [], []\n",
    "    \n",
    "    # optimizador\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # ciclo de entrenamiento\n",
    "    for epoch in trange(epochs):\n",
    "\n",
    "        # entrenamos la época\n",
    "        train_epoch_GPU(trn_dl, model, opt)\n",
    "\n",
    "        # evaluamos la época en entrenamiento\n",
    "        trn_loss, trn_acc = eval_epoch_GPU(trn_dl, model, trn_batches)\n",
    "        # evaluamos la época en prueba\n",
    "        tst_loss, tst_acc = eval_epoch_GPU(tst_dl, model, tst_batches)\n",
    "\n",
    "        # guardamos historial\n",
    "        loss_hist.append([trn_loss, tst_loss])\n",
    "        acc_hist.append([trn_acc, tst_acc])\n",
    "\n",
    "        # imprimimos progreso\n",
    "        print(f'E{epoch:02} '\n",
    "              f'loss=[{trn_loss:6.2f},{tst_loss:6.2f}] '\n",
    "              f'acc=[{trn_acc:5.2f},{tst_acc:5.2f}]')\n",
    "\n",
    "    return loss_hist, acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a9d13458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:31<02:05, 31.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E00 loss=[ 52.91, 86.70] acc=[71.82,53.94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:03<01:35, 31.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E01 loss=[ 37.94, 95.51] acc=[80.63,54.44]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:35<01:03, 31.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E02 loss=[ 26.98,103.85] acc=[89.91,54.94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [02:09<00:32, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E03 loss=[ 17.40,105.09] acc=[94.62,56.81]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:48<00:00, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E04 loss=[ 13.83,114.33] acc=[96.29,57.45]\n",
      "CPU times: user 11min 32s, sys: 10min, total: 21min 32s\n",
      "Wall time: 2min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# instanciamos un modelo\n",
    "model = CNN()\n",
    "# entrenamos\n",
    "loss_hist, acc_hist = train_GPU(model, trn_dl, tst_dl, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64974186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
