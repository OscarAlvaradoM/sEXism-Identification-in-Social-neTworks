{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fd91c6",
   "metadata": {},
   "source": [
    "# Aprendizaje profundo para detección de sexismo\n",
    "- Óscar Alvarado\n",
    "- Dante Bermúdez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7895a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from itertools import islice as take\n",
    "import torch\n",
    "from torch import nn\n",
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# barras de progreso\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56910979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76aa033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colapsar_repeticion(match):\n",
    "    elemento = match.groups()\n",
    "    assert len(match.groups()) == 1\n",
    "    return elemento[0]\n",
    "\n",
    "def procesar_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    regex_usuario = re.compile(r\"@[\\w\\d]+\")\n",
    "    tweet = regex_usuario.sub(\"@usuario\", tweet)\n",
    "    \n",
    "    regex_link = re.compile(r\"\\b(?:https?://|www\\.)\\S+\\b\")\n",
    "    tweet = regex_link.sub(\"<link>\", tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    regex_collapse = re.compile(r\"(\\w)\\1{2}\")\n",
    "    \n",
    "    tokens = [regex_collapse.sub(colapsar_repeticion, token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948e7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8ca64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>She calls herself \"anti-feminazi\" how about sh...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>2</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>Now, back to these women, the brave and the be...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@CurvyBandida @Xalynne_B Wow, your skirt is ve...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>4</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@AurelieGuiboud Incredible!  Beautiful!But I l...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>5</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>i find it extremely hard to believe that kelly...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_case  id   source language  \\\n",
       "0  EXIST2021   1  twitter       en   \n",
       "1  EXIST2021   2  twitter       en   \n",
       "2  EXIST2021   3  twitter       en   \n",
       "3  EXIST2021   4  twitter       en   \n",
       "4  EXIST2021   5  twitter       en   \n",
       "\n",
       "                                                text       task1  \\\n",
       "0  She calls herself \"anti-feminazi\" how about sh...      sexist   \n",
       "1  Now, back to these women, the brave and the be...  non-sexist   \n",
       "2  @CurvyBandida @Xalynne_B Wow, your skirt is ve...      sexist   \n",
       "3  @AurelieGuiboud Incredible!  Beautiful!But I l...  non-sexist   \n",
       "4  i find it extremely hard to believe that kelly...  non-sexist   \n",
       "\n",
       "                    task2  \n",
       "0  ideological-inequality  \n",
       "1              non-sexist  \n",
       "2         objectification  \n",
       "3              non-sexist  \n",
       "4              non-sexist  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../datos/training/EXIST2021_training.tsv\", sep=\"\\t\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ccd32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_train[\"text\"].apply(procesar_tweet)\n",
    "labels1 = df_train[\"task1\"].map({\"sexist\":1, \"non-sexist\":0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03522c0",
   "metadata": {},
   "source": [
    "## Train - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a220f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets, labels1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8578813e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5581x8424 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 135091 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=2, tokenizer=lambda x:x, preprocessor=lambda x:x)\n",
    "vect.fit_transform(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc482f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5714, 8240, 1645, 4982, 4189, 6389, 5405, 1, 7529, 8234, 4183, 7533, 5429, 5425, 6243, 5462, 3990, 2155, 327]\n"
     ]
    }
   ],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(vect.get_feature_names(), 2)}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<START>\"] = 1\n",
    "X_train = [[word2idx.get(word, 1) for word in tweet] for tweet in train_tweets]\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677e4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6794, 397, 7632, 4395, 5108, 7585, 6261, 7469, 4615, 4989, 2365, 2213, 5588, 2313, 2927, 6957, 6308, 418, 276, 770, 5261, 276, 8314, 3594, 2213, 2365, 1, 2618, 8312, 7638, 4615, 6261, 6748, 6308, 1, 2227, 7469, 4615, 4989, 2365, 276, 7011, 657, 6748, 5099, 5946, 7648, 2084, 1985]\n"
     ]
    }
   ],
   "source": [
    "X_test = [[word2idx.get(word, 1) for word in tweet] for tweet in test_tweets]\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d8d75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(X) for X in X_train])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dedbf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8426"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palabras en el vocabulario\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9730f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "for idx, X in enumerate(X_train):\n",
    "    zeros = [0 for _ in range(max_len)]\n",
    "    len_x = len(X)\n",
    "    zeros[-len_x:] = X\n",
    "    X_train[idx] = zeros\n",
    "    \n",
    "for idx, X in enumerate(X_test):\n",
    "    zeros = [0 for _ in range(max_len)]\n",
    "    len_x = len(X)\n",
    "    zeros[-len_x:] = X\n",
    "    X_test[idx] = zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367007a",
   "metadata": {},
   "source": [
    "## Arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c880b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de arquitectura\n",
    "class CNN(nn.Module):    \n",
    "    def __init__(self, num_labels=2):\n",
    "        super(CNN, self).__init__()\n",
    "        # Valores iniciales\n",
    "        num_embeddings = 8438\n",
    "        embedding_dim = 50\n",
    "        kernels = 30\n",
    "        k_cnn = 9\n",
    "        pad_cnn = 0\n",
    "        dilation_cnn = 1\n",
    "        step_cnn = 1\n",
    "        k_pool = 9\n",
    "        pad_pool = 0\n",
    "        dilation_pool = 1\n",
    "        step_pool = 1\n",
    "        \n",
    "        # Capa para Embeddings\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim) # 8438 palabras en el vocabulario, embedding 50-dimensional\n",
    "        \n",
    "        # Capa convolucional\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = embedding_dim, out_channels = kernels, kernel_size = k_cnn, padding = pad_cnn,\n",
    "                     dilation = dilation_cnn, stride = step_cnn),\n",
    "            # Función de activación\n",
    "            nn.ReLU(),\n",
    "            # Pooling\n",
    "            nn.MaxPool1d(kernel_size = 9, padding = pad_pool,\n",
    "                     dilation = dilation_pool, stride = step_pool))\n",
    "        \n",
    "        # Aplanado\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculando el número de características\n",
    "        out_cnn = int((110 + 2*pad_cnn - dilation_cnn*(k_cnn - 1) - 1)/step_cnn) + 1\n",
    "        out_pool = int((out_cnn + 2*pad_pool - dilation_pool*(k_pool - 1) - 1)/step_pool) + 1\n",
    "        \n",
    "        self.num_features = kernels*out_pool\n",
    "        \n",
    "        #Clasificación\n",
    "        self.cls = nn.Linear(self.num_features, num_labels)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # metodo para inferencia\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.cls(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30568414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (emb): Embedding(8438, 50)\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(50, 30, kernel_size=(9,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=9, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (cls): Linear(in_features=2820, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f921766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 110]) => torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "# inferencia con datos sintéticos\n",
    "x = torch.tensor([X_train[0]])\n",
    "y = model(x)\n",
    "print(f'{x.shape} => {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fac91d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.Tensor(X_train).to(torch.int64) # transform to torch tensor\n",
    "y_train_t = torch.Tensor(train_labels).to(torch.int64)\n",
    "\n",
    "trn_dataset = TensorDataset(X_train_t, y_train_t) # create your datset\n",
    "trn_dl = DataLoader(trn_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f45f9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t = torch.Tensor(X_test).to(torch.int64) # transform to torch tensor\n",
    "y_test_t = torch.Tensor(test_labels).to(torch.int64)\n",
    "\n",
    "tst_dataset = TensorDataset(X_test_t, y_test_t) # create your datset\n",
    "tst_dl = DataLoader(tst_dataset) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f700556",
   "metadata": {},
   "source": [
    "## Modelo paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c1a2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_GPU(dl, model, opt):\n",
    "\n",
    "    # por cada lote\n",
    "    for x, y_true in dl:\n",
    "        \n",
    "        # computamos logits\n",
    "        y_lgts = model(x.to(torch.int64))\n",
    "        \n",
    "        # computamos la pérdida\n",
    "        loss = F.cross_entropy(y_lgts, y_true)\n",
    "        \n",
    "        # vaciamos los gradientes\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # retropropagamos\n",
    "        loss.backward()\n",
    "        \n",
    "        # actualizamos parámetros\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "def eval_epoch_GPU(dl, model, num_batches=None):\n",
    "\n",
    "    # evitamos que se registren las operaciones \n",
    "    # en la gráfica de cómputo\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # historiales\n",
    "        losses, accs = [], []\n",
    "\n",
    "        # validación de la época con num_batches\n",
    "        # si num_batches==None, se usan todos los lotes\n",
    "        for x, y_true in take(dl, num_batches):\n",
    "\n",
    "            # computamos los logits\n",
    "            y_lgts = model(x)\n",
    "\n",
    "            # computamos los puntajes\n",
    "            y_prob = F.softmax(y_lgts, 1)\n",
    "\n",
    "            # computamos la clases\n",
    "            y_pred = torch.argmax(y_prob, 1)\n",
    "\n",
    "            # computamos la pérdida\n",
    "            loss = F.cross_entropy(y_lgts, y_true)\n",
    "\n",
    "            # computamos la exactitud\n",
    "            acc = (y_true == y_pred).type(torch.float32).mean()\n",
    "\n",
    "            # guardamos históricos\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc.item())\n",
    "\n",
    "        # promediamos\n",
    "        loss = np.mean(losses) * 100\n",
    "        acc = np.mean(accs) * 100\n",
    "\n",
    "        return loss, acc\n",
    "        \n",
    "        \n",
    "def train_GPU(model, trn_dl, tst_dl, lr=1e-3, epochs=20,\n",
    "          trn_batches=None, tst_batches=None):\n",
    "\n",
    "    # historiales\n",
    "    loss_hist, acc_hist = [], []\n",
    "    \n",
    "    # optimizador\n",
    "    opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # ciclo de entrenamiento\n",
    "    for epoch in trange(epochs):\n",
    "\n",
    "        # entrenamos la época\n",
    "        train_epoch_GPU(trn_dl, model, opt)\n",
    "\n",
    "        # evaluamos la época en entrenamiento\n",
    "        trn_loss, trn_acc = eval_epoch_GPU(trn_dl, model, trn_batches)\n",
    "        # evaluamos la época en prueba\n",
    "        tst_loss, tst_acc = eval_epoch_GPU(tst_dl, model, tst_batches)\n",
    "\n",
    "        # guardamos historial\n",
    "        loss_hist.append([trn_loss, tst_loss])\n",
    "        acc_hist.append([trn_acc, tst_acc])\n",
    "\n",
    "        # imprimimos progreso\n",
    "        print(f'E{epoch:02} '\n",
    "              f'loss=[{trn_loss:6.2f},{tst_loss:6.2f}] '\n",
    "              f'acc=[{trn_acc:5.2f},{tst_acc:5.2f}]')\n",
    "\n",
    "    return loss_hist, acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b08d3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:08<00:33,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E00 loss=[ 65.68, 72.80] acc=[57.43,51.36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:15<00:23,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E01 loss=[ 52.20, 74.37] acc=[73.89,54.80]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:23<00:15,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E02 loss=[ 40.03, 83.67] acc=[81.87,54.23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:30<00:07,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E03 loss=[ 30.13, 97.88] acc=[87.49,53.65]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:38<00:00,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E04 loss=[ 19.79,108.58] acc=[92.56,53.51]\n",
      "CPU times: user 2min 22s, sys: 2min 36s, total: 4min 59s\n",
      "Wall time: 38.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# instanciamos un modelo\n",
    "model = CNN()\n",
    "# entrenamos\n",
    "loss_hist, acc_hist = train_GPU(model, trn_dl, tst_dl, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fb824c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
