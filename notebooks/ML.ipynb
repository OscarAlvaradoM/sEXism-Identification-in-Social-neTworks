{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "practical-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualización\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold, train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "associate-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colapsar_repeticion(match):\n",
    "    elemento = match.groups()\n",
    "    assert len(match.groups()) == 1\n",
    "    return elemento[0]\n",
    "\n",
    "def procesar_tweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    regex_usuario = re.compile(r\"@[\\w\\d]+\")\n",
    "    tweet = regex_usuario.sub(\"@usuario\", tweet)\n",
    "    \n",
    "    regex_link = re.compile(r\"\\b(?:https?://|www\\.)\\S+\\b\")\n",
    "    tweet = regex_link.sub(\"<link>\", tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer(reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    regex_collapse = re.compile(r\"(\\w)\\1{2}\")\n",
    "    \n",
    "    tokens = [regex_collapse.sub(colapsar_repeticion, token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variable-joshua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>She calls herself \"anti-feminazi\" how about sh...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>2</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>Now, back to these women, the brave and the be...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@CurvyBandida @Xalynne_B Wow, your skirt is ve...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>4</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@AurelieGuiboud Incredible!  Beautiful!But I l...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>5</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>i find it extremely hard to believe that kelly...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6972</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6973</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>Estamos igual sin pareja, pero puedes besar a ...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6973</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6974</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>2020 hijo de re mil putas</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6974</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6975</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>SEGURAMENTE ESTA CHICA NO COBRA EL DINERO QUE ...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6976</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>@safetyaitana mi madre dice q va fea y i agree</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6976</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6977</td>\n",
       "      <td>twitter</td>\n",
       "      <td>es</td>\n",
       "      <td>¿En vuestras casas también tenéis esa tradició...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>stereotyping-dominance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6977 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      test_case    id   source language  \\\n",
       "0     EXIST2021     1  twitter       en   \n",
       "1     EXIST2021     2  twitter       en   \n",
       "2     EXIST2021     3  twitter       en   \n",
       "3     EXIST2021     4  twitter       en   \n",
       "4     EXIST2021     5  twitter       en   \n",
       "...         ...   ...      ...      ...   \n",
       "6972  EXIST2021  6973  twitter       es   \n",
       "6973  EXIST2021  6974  twitter       es   \n",
       "6974  EXIST2021  6975  twitter       es   \n",
       "6975  EXIST2021  6976  twitter       es   \n",
       "6976  EXIST2021  6977  twitter       es   \n",
       "\n",
       "                                                   text       task1  \\\n",
       "0     She calls herself \"anti-feminazi\" how about sh...      sexist   \n",
       "1     Now, back to these women, the brave and the be...  non-sexist   \n",
       "2     @CurvyBandida @Xalynne_B Wow, your skirt is ve...      sexist   \n",
       "3     @AurelieGuiboud Incredible!  Beautiful!But I l...  non-sexist   \n",
       "4     i find it extremely hard to believe that kelly...  non-sexist   \n",
       "...                                                 ...         ...   \n",
       "6972  Estamos igual sin pareja, pero puedes besar a ...  non-sexist   \n",
       "6973                          2020 hijo de re mil putas  non-sexist   \n",
       "6974  SEGURAMENTE ESTA CHICA NO COBRA EL DINERO QUE ...  non-sexist   \n",
       "6975     @safetyaitana mi madre dice q va fea y i agree      sexist   \n",
       "6976  ¿En vuestras casas también tenéis esa tradició...      sexist   \n",
       "\n",
       "                       task2  \n",
       "0     ideological-inequality  \n",
       "1                 non-sexist  \n",
       "2            objectification  \n",
       "3                 non-sexist  \n",
       "4                 non-sexist  \n",
       "...                      ...  \n",
       "6972              non-sexist  \n",
       "6973              non-sexist  \n",
       "6974              non-sexist  \n",
       "6975         objectification  \n",
       "6976  stereotyping-dominance  \n",
       "\n",
       "[6977 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../datos/training/EXIST2021_training.tsv\", sep=\"\\t\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "immediate-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_train[\"text\"].apply(procesar_tweet)\n",
    "labels1 = df_train[\"task1\"].map({\"sexist\":1, \"non-sexist\":0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-union",
   "metadata": {},
   "source": [
    "# Train - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "looking-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(tweets, labels1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "coordinated-productivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5581"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "finite-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, tokenizer=lambda x:x, preprocessor=lambda x:x)\n",
    "X_train = vectorizer.fit_transform(train_tweets)\n",
    "y_train = train_labels\n",
    "X_test = vectorizer.transform(test_tweets)\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "willing-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar_modelo(Modelo, model_params, X_train, X_test, y_train, y_test, \n",
    "                    search_space, n_iter):\n",
    "    # Entrenando y optimizando\n",
    "    model = Modelo(**model_params)\n",
    "    tunned_model = RandomizedSearchCV(model, search_space, n_iter=50, n_jobs=-1)\n",
    "    tunned_model.fit(X_train, y_train)\n",
    "    print(tunned_model.best_params_)\n",
    "    \n",
    "    # Evaluación\n",
    "    y_pred = tunned_model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return tunned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "greenhouse-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 4.693727617375979, 'penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       763\n",
      "           1       0.70      0.70      0.70       633\n",
      "\n",
      "    accuracy                           0.73      1396\n",
      "   macro avg       0.73      0.73      0.73      1396\n",
      "weighted avg       0.73      0.73      0.73      1396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_space = {\"penalty\":[\"l1\", \"l2\"], \n",
    "                \"C\":stats.uniform(1, 99)}\n",
    "model_lr = entrenar_modelo(LogisticRegression, {\"max_iter\":500, \"random_state\":42}, \n",
    "                            *dataset, search_space, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "younger-respect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../modelos/logistic_regression.joblib']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(modelo_lr, \"../modelos/logistic_regression.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "based-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dante/miniconda3/envs/pln/lib/python3.8/site-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 33.49372824812848, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63       763\n",
      "           1       0.56      0.58      0.57       633\n",
      "\n",
      "    accuracy                           0.60      1396\n",
      "   macro avg       0.60      0.60      0.60      1396\n",
      "weighted avg       0.61      0.60      0.60      1396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_space = {\"kernel\":[\"rbf\", \"sigmoid\", \"poly\"], \n",
    "                \"gamma\":[\"scale\", \"auto\"], \n",
    "                \"degree\":[2,3,4],\n",
    "                \"C\":stats.uniform(0.1, 99.9)}\n",
    "model_svc = entrenar_modelo(SVC, {\"max_iter\":500, \"random_state\":42}, \n",
    "                            *dataset, search_space, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "compound-bradley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 59, 'max_features': None, 'min_samples_split': 2}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.71       763\n",
      "           1       0.66      0.61      0.63       633\n",
      "\n",
      "    accuracy                           0.68      1396\n",
      "   macro avg       0.68      0.67      0.67      1396\n",
      "weighted avg       0.68      0.68      0.68      1396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_space = {\"criterion\" : [\"gini\", \"entropy\"], \n",
    "                \"max_depth\" : stats.randint(50, 201), \n",
    "                \"max_features\" : [None, \"sqrt\", \"log2\"],\n",
    "                \"min_samples_split\" : stats.randint(2, 11)}\n",
    "model_dtc = entrenar_modelo(DecisionTreeClassifier, {\"random_state\":42}, *dataset, search_space, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cubic-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 137, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 159}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74       763\n",
      "           1       0.69      0.68      0.69       633\n",
      "\n",
      "    accuracy                           0.72      1396\n",
      "   macro avg       0.72      0.71      0.71      1396\n",
      "weighted avg       0.72      0.72      0.72      1396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_space = {\"n_estimators\" : stats.randint(80, 201),\n",
    "                \"criterion\" : [\"gini\", \"entropy\"], \n",
    "                \"max_depth\" : stats.randint(50, 201), \n",
    "                \"max_features\" : [None, \"sqrt\", \"log2\"],\n",
    "                \"min_samples_split\" : stats.randint(2, 11)}\n",
    "model_rfc = entrenar_modelo(RandomForestClassifier, {\"random_state\":42}, *dataset, search_space, n_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "governmental-specialist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../modelos/random_forest.joblib']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_rfc, \"../modelos/random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "silver-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 7.831687642388317, 'penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75       763\n",
      "           1       0.70      0.69      0.69       633\n",
      "\n",
      "    accuracy                           0.72      1396\n",
      "   macro avg       0.72      0.72      0.72      1396\n",
      "weighted avg       0.72      0.72      0.72      1396\n",
      "\n",
      "CPU times: user 984 ms, sys: 28 ms, total: 1.01 s\n",
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Entrenamos modelo\n",
    "Modelo = LogisticRegression\n",
    "model_params = {\"max_iter\":500}\n",
    "model = Modelo(**model_params)\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "search_space = {\"penalty\":[\"l1\", \"l2\"], \n",
    "                \"C\":stats.uniform(1, 99)}\n",
    "tunned_model = RandomizedSearchCV(model, search_space, n_iter=50, n_jobs=-1)\n",
    "tunned_model.fit(X_train, y_train)\n",
    "print(tunned_model.best_params_)\n",
    "\n",
    "# Evaluación en conjunto de prueba\n",
    "y_pred = tunned_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-sweden",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "decreased-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_training, tweets_test, y_training, y_test = train_test_split(tweets, labels1, test_size=0.2)\n",
    "tweets_train, tweets_val, y_train, y_val = train_test_split(tweets_training, y_training, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "prompt-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, tokenizer=lambda x:x, preprocessor=lambda x:x)\n",
    "X_train = vectorizer.fit_transform(tweets_train)\n",
    "X_val = vectorizer.transform(tweets_val)\n",
    "X_test = vectorizer.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "recovered-thinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminó entrenamiento\n",
      "{'C': 4.367401011018379, 'penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       704\n",
      "           1       0.69      0.69      0.69       692\n",
      "\n",
      "    accuracy                           0.69      1396\n",
      "   macro avg       0.69      0.69      0.69      1396\n",
      "weighted avg       0.69      0.69      0.69      1396\n",
      "\n",
      "CPU times: user 1.15 s, sys: 76 µs, total: 1.15 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Entrenamos modelo\n",
    "Modelo = LogisticRegression\n",
    "model_params = {\"max_iter\":500}\n",
    "model = Modelo(**model_params)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Terminó entrenamiento\")\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "search_space = {\"penalty\":[\"l1\", \"l2\"], \n",
    "                \"C\":stats.uniform(1, 99)}\n",
    "tunned_model = RandomizedSearchCV(model, search_space, n_iter=50, n_jobs=-1)\n",
    "tunned_model.fit(X_val, y_val)\n",
    "print(tunned_model.best_params_)\n",
    "\n",
    "# Evaluación en conjunto de prueba\n",
    "y_pred = tunned_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-gauge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
